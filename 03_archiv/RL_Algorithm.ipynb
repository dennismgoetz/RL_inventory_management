{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "88d75a9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'arg1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4k/d6yzgkwn1v94c9ny5xtnpc700000gn/T/ipykernel_90183/3035884345.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.17\u001b[0m  \u001b[0;31m#learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m \u001b[0;31m#exploration probaility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mq_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#sparse Q-table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mlast_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mlast_action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'arg1'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "gamma = 1     #discount rate\n",
    "alpha = 0.17  #learning rate\n",
    "epsilon = 0.1 #exploration probaility\n",
    "q_table = csr_matrix(shape=None, dtype=None) #sparse Q-table\n",
    "last_state\n",
    "last_action\n",
    "\n",
    "# read the last action value from the q table\n",
    "last_action_value = self.q_table.read_value(\n",
    "        self.last_state,\n",
    "        self.last_action\n",
    ")\n",
    "\n",
    "# define the current state, which is the last order balance\n",
    "# modified by the incoming order\n",
    "\n",
    "current_state = self.agent.order_balance-self.agent.incoming_order\n",
    "\n",
    "# work out the next action value for the current state, which is the order\n",
    "# balance minus the new incoming order\n",
    "\n",
    "max_next_action_value = self.q_table.max_action_value((current_state,))\n",
    "    \n",
    "# work out the new action value based on the q-learning algorithm introduced above \n",
    "# - this is necessary because the reward is calculated at the end of the step,\n",
    "# i.e the update can only be done in the next timestep.\n",
    "\n",
    "new_action_value = (\n",
    "    last_action_value +\n",
    "    self.agent.model.alpha*(\n",
    "        self.agent.reward + \n",
    "        self.agent.model.gamma * max_next_action_value -\n",
    "        last_action_value\n",
    "    )\n",
    ")\n",
    "\n",
    "# update the action value for the last state and the last action\n",
    "    \n",
    "self.q_table.add_value(self.last_state, self.last_action, new_action_value)\n",
    "    \n",
    "# now decide  on which action to take, either by looking up the best value \n",
    "# in the q-table or by choosing a random number (ùúñ-greedy approach)\n",
    "\n",
    "if random.uniform(0, 1) < self.agent.model.epsilon:\n",
    "    # once in a while we choose a random amount to ensure\n",
    "    # we are not locked into a local maximum\n",
    "    order_amount =  random.randint(0, round(1.5*self.agent.incoming_order)) \n",
    "else:\n",
    "    # look up the best action for this state in the q-table\n",
    "    order_amount = self.q_table.best_action((current_state,))\n",
    "            \n",
    "# now remember the state and action taken,\n",
    "# as a starting point for the next round\n",
    "self.last_state = (current_state,)\n",
    "self.last_action = order_amount\n",
    "\n",
    "end_round\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "23963168",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4k/d6yzgkwn1v94c9ny5xtnpc700000gn/T/ipykernel_90183/4085161930.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Rewardfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder_balance\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder_balance\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over_round\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# run one more round to pickup the rewards, then stop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "#Rewardfunction\n",
    "if (agent.order_balance < 0 or agent.order_balance > 1400):\n",
    "    self.game_over = True\n",
    "    self.game_over_round = time + 1\n",
    "# run one more round to pickup the rewards, then stop\n",
    "    reward = -10000\n",
    "elif agent.outgoing_order < agent.incoming_order:\n",
    "    reward = -10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d24e69ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4k/d6yzgkwn1v94c9ny5xtnpc700000gn/T/ipykernel_90183/2728519408.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Define milestones to keep q_table small\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;36m750\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder_balance\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "#Define milestones to keep q_table small\n",
    "if time == 3:\n",
    "                    if 750 >= agent.order_balance >= 600:\n",
    "                            reward += 2000\n",
    "                    if time == 5:\n",
    "                        if 900 >= agent.order_balance >= 700:\n",
    "                            reward += 2000      \n",
    "                    if time == 10:\n",
    "                        if 1150 >= agent.order_balance >= 1000:\n",
    "                            reward += 2000      \n",
    "                    if time == 15:\n",
    "                        if 1250 >= agent.order_balance >= 1100:\n",
    "                            reward += 2000\n",
    "                    if time == 20:\n",
    "                        if 1250 >= agent.order_balance >= 1150:\n",
    "                            reward += 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "04a5283b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bptk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4k/d6yzgkwn1v94c9ny5xtnpc700000gn/T/ipykernel_90183/1657011016.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 10 Iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m bptk.train_simulations(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mscenario_managers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"smBeergameQlOB\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mscenarios\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_agents\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bptk' is not defined"
     ]
    }
   ],
   "source": [
    "# 10 Iterations\n",
    "bptk.train_simulations(\n",
    "    episodes=10,\n",
    "    scenario_managers=[\"smBeergameQlOB\"],\n",
    "    scenarios=[\"train_agents\"],\n",
    "    agents=[\"controlling\"],\n",
    "    agent_states=[\"active\"],\n",
    "    agent_properties=[\"supply_chain_reward\"],\n",
    "    agent_property_types=[\"total\"],\n",
    "    return_df=False,\n",
    "    progress_bar=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
